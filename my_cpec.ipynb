{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Admin'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('second.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rise of China and the Future of the West</td>\n",
       "      <td>China's rise will inevitably bring the United ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How Russia and China Undermine Democracy</td>\n",
       "      <td>Both Russia and China view weakening Western d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When China Rules the Web</td>\n",
       "      <td>China is set to remake cyberspace in its own i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reeducation Returns to China</td>\n",
       "      <td>It is possible to see how Xinjiang’s reeducati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China Courts Iran</td>\n",
       "      <td>As the Trump administration works out the spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Will China Undermine Trump's Iran Strategy?</td>\n",
       "      <td>For the better part of two decades, Iran’s lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A Counterproductive Cold War With China</td>\n",
       "      <td>Trump's vision for a “free and open Indo-Pacif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>China and the World</td>\n",
       "      <td>Washington should learn from its embarrassing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Did America Get China Wrong?</td>\n",
       "      <td>Experts from both the United States and China ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trump and China</td>\n",
       "      <td>Neither China nor the United States would bene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              h  \\\n",
       "0  The Rise of China and the Future of the West   \n",
       "1      How Russia and China Undermine Democracy   \n",
       "2                      When China Rules the Web   \n",
       "3                  Reeducation Returns to China   \n",
       "4                             China Courts Iran   \n",
       "5   Will China Undermine Trump's Iran Strategy?   \n",
       "6       A Counterproductive Cold War With China   \n",
       "7                           China and the World   \n",
       "8                  Did America Get China Wrong?   \n",
       "9                               Trump and China   \n",
       "\n",
       "                                                   b  \n",
       "0  China's rise will inevitably bring the United ...  \n",
       "1  Both Russia and China view weakening Western d...  \n",
       "2  China is set to remake cyberspace in its own i...  \n",
       "3  It is possible to see how Xinjiang’s reeducati...  \n",
       "4  As the Trump administration works out the spec...  \n",
       "5  For the better part of two decades, Iran’s lea...  \n",
       "6  Trump's vision for a “free and open Indo-Pacif...  \n",
       "7  Washington should learn from its embarrassing ...  \n",
       "8  Experts from both the United States and China ...  \n",
       "9  Neither China nor the United States would bene...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                               h  \\\n",
       "0  The Rise of China and the Future of the West   \n",
       "1      How Russia and China Undermine Democracy   \n",
       "2                      When China Rules the Web   \n",
       "3                  Reeducation Returns to China   \n",
       "4                             China Courts Iran   \n",
       "5   Will China Undermine Trump's Iran Strategy?   \n",
       "6       A Counterproductive Cold War With China   \n",
       "7                           China and the World   \n",
       "8                  Did America Get China Wrong?   \n",
       "9                               Trump and China   \n",
       "\n",
       "                                                   b  \n",
       "0  China's rise will inevitably bring the United ...  \n",
       "1  Both Russia and China view weakening Western d...  \n",
       "2  China is set to remake cyberspace in its own i...  \n",
       "3  It is possible to see how Xinjiang’s reeducati...  \n",
       "4  As the Trump administration works out the spec...  \n",
       "5  For the better part of two decades, Iran’s lea...  \n",
       "6  Trump's vision for a “free and open Indo-Pacif...  \n",
       "7  Washington should learn from its embarrassing ...  \n",
       "8  Experts from both the United States and China ...  \n",
       "9  Neither China nor the United States would bene...  >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   \n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    China's rise will inevitably bring the United ...\n",
       "1    Both Russia and China view weakening Western d...\n",
       "2    China is set to remake cyberspace in its own i...\n",
       "3    It is possible to see how Xinjiang’s reeducati...\n",
       "4    As the Trump administration works out the spec...\n",
       "5    For the better part of two decades, Iran’s lea...\n",
       "6    Trump's vision for a “free and open Indo-Pacif...\n",
       "7    Washington should learn from its embarrassing ...\n",
       "8    Experts from both the United States and China ...\n",
       "9    Neither China nor the United States would bene...\n",
       "Name: b, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file1=file.iloc[:,0:2]\n",
    "#file1\n",
    "file['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=file['b']\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
    "\n",
    "y=list()\n",
    "for headings in x:\n",
    "    xt = tokenizer.tokenize(headings)\n",
    "    y.extend(xt)\n",
    "\n",
    "#x = tokenizer.tokenize(x.decode(\"utf8\"))\n",
    "# no need to decode since we did not make any byte to integer consversion.\n",
    "#y=list()\n",
    "#for x in file['h']:\n",
    "    #y.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"China's\", 'rise', 'will', 'inevitably', 'bring', 'the', 'United', \"States'\", 'unipolar', 'moment', 'to', 'an', 'end', 'But', 'that', 'does', 'not', 'necessarily', 'mean', 'a', 'violent', 'power', 'struggle', 'or', 'the', 'overthrow', 'of', 'the', 'Both', 'Russia', 'and', 'China', 'view', 'weakening', 'Western', 'democracy', 'as', 'a', 'means', 'of', 'enhancing', 'their', 'own', 'standing', 'China', 'is', 'set', 'to', 'remake', 'cyberspace', 'in', 'its', 'own', 'image', 'That', 'will', 'make', 'the', 'Internet', 'less', 'open', 'and', 'allow', 'Beijing', 'to', 'reap', 'vast', 'economic', 'diplomatic', 'and', 'security', 'benefits', 'It', 'is', 'possible', 'to', 'see', 'how', 'Xinjiang', 's', 'reeducation', 'drive', 'could', 'end', 'up', 'influencing', 'the', 'nation', 's', 'future', 'social', 'credit', 'system', 'those', 'who', 'end', 'up', 'falling', 'below', 'a', 'certain', 'As', 'the', 'Trump', 'administration', 'works', 'out', 'the', 'specifics', 'of', 'its', 'strategy', 'to', 'contain', 'Iran', 'China', 'is', 'looking', 'for', 'ways', 'to', 'bring', 'Iran', 'into', 'the', 'global', 'system', 'For', 'the', 'better', 'part', 'of', 'two', 'decades', 'Iran', 's', 'leadership', 'has', 'been', 'hedging', 'against', 'international', 'isolation', 'by', 'developing', 'deeper', 'ties', 'with', 'China', 'and', 'Russia', \"Trump's\", 'vision', 'for', 'a', 'free', 'and', 'open', 'Indo', 'Pacific', 'will', 'likely', 'provoke', 'Beijing', 'alarm', 'other', 'Asian', 'nations', 'and', 'drive', 'the', 'region', 'toward', 'a', 'highly', 'tense', 'zero', 'sum', 'Washington', 'should', 'learn', 'from', 'its', 'embarrassing', 'and', 'futile', 'attempt', 'to', 'block', 'the', 'AIIB', 'The', 'best', 'way', 'to', 'bolster', 'international', 'order', 'and', 'adapt', 'to', 'China', 's', 'increasing', 'Experts', 'from', 'both', 'the', 'United', 'States', 'and', 'China', 'weigh', 'in', 'on', 'Kurt', 'Campbell', 'and', 'Ely', 'Ratner', 's', 'March', 'April', 'article', 'about', 'U', 'S', 'China', 'policy', 'Neither', 'China', 'nor', 'the', 'United', 'States', 'would', 'benefit', 'from', 'a', 'trade', 'war', 'an', 'arms', 'race', 'or', 'a', 'military', 'confrontation', 'So', 'Washington', 'should', 'stick', 'to', 'its', 'cautious', 'balanced']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#corpus = file['col1']\n",
    "corpus=y\n",
    "vectorizer = CountVectorizer(analyzer = stem_text, stop_words = 'enlgish')\n",
    "#X_counts = vectorizer.fit_transform(corpus)\n",
    "#vectorizer.fit_transform(corpus)\n",
    "#print(X_counts)\n",
    "#X_counts.data \n",
    "#print(X_counts.shape)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(X_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in text])\n",
    "    text = [ps.stem(word) for word in filtered_text]    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 224)\n",
      "['about china', 'adapt to', 'administration works', 'against international', 'aiib the', 'alarm other', 'allow beijing', 'an arms', 'an end', 'and adapt', 'and allow', 'and china', 'and drive', 'and ely', 'and futile', 'and open', 'and russia', 'and security', 'april article', 'arms race', 'article about', 'as means', 'as the', 'asian nations', 'attempt to', 'been hedging', 'beijing alarm', 'beijing to', 'below certain', 'benefit from', 'best way', 'better part', 'block the', 'bolster international', 'both russia', 'both the', 'bring iran', 'bring the', 'but that', 'by developing', 'campbell and', 'cautious balanced', 'china and', 'china increasing', 'china is', 'china nor', 'china policy', 'china rise', 'china view', 'china weigh', 'confrontation so', 'contain iran', 'could end', 'credit system', 'cyberspace in', 'decades iran', 'deeper ties', 'democracy as', 'developing deeper', 'diplomatic and', 'does not', 'drive could', 'drive the', 'economic diplomatic', 'ely ratner', 'embarrassing and', 'end but', 'end up', 'enhancing their', 'experts from', 'falling below', 'for free', 'for the', 'for ways', 'free and', 'from both', 'from its', 'from trade', 'futile attempt', 'future social', 'global system', 'has been', 'hedging against', 'highly tense', 'how xinjiang', 'image that', 'in its', 'in on', 'indo pacific', 'inevitably bring', 'influencing the', 'international isolation', 'international order', 'internet less', 'into the', 'iran china', 'iran into', 'iran leadership', 'is looking', 'is possible', 'is set', 'isolation by', 'it is', 'its cautious', 'its embarrassing', 'its own', 'its strategy', 'kurt campbell', 'leadership has', 'learn from', 'less open', 'likely provoke', 'looking for', 'make the', 'march april', 'mean violent', 'means of', 'military confrontation', 'moment to', 'nation future', 'nations and', 'necessarily mean', 'neither china', 'nor the', 'not necessarily', 'of enhancing', 'of its', 'of the', 'of two', 'on kurt', 'open and', 'open indo', 'or military', 'or the', 'order and', 'other asian', 'out the', 'overthrow of', 'own image', 'own standing', 'pacific will', 'part of', 'possible to', 'power struggle', 'provoke beijing', 'race or', 'ratner march', 'reap vast', 'reeducation drive', 'region toward', 'remake cyberspace', 'rise will', 'russia and', 'security benefits', 'see how', 'set to', 'should learn', 'should stick', 'so washington', 'social credit', 'specifics of', 'states and', 'states unipolar', 'states would', 'stick to', 'strategy to', 'struggle or', 'system those', 'tense zero', 'that does', 'that will', 'the aiib', 'the best', 'the better', 'the global', 'the internet', 'the nation', 'the overthrow', 'the region', 'the specifics', 'the trump', 'the united', 'their own', 'those who', 'ties with', 'to an', 'to block', 'to bolster', 'to bring', 'to china', 'to contain', 'to its', 'to reap', 'to remake', 'to see', 'toward highly', 'trade war', 'trump administration', 'trump vision', 'two decades', 'unipolar moment', 'united states', 'up falling', 'up influencing', 'vast economic', 'view weakening', 'violent power', 'vision for', 'war an', 'washington should', 'way to', 'ways to', 'weakening western', 'weigh in', 'western democracy', 'who end', 'will inevitably', 'will likely', 'will make', 'with china', 'works out', 'would benefit', 'xinjiang reeducation', 'zero sum']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = [[word.lower() for word in line.split()] for line in file['h']]\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2)) #analyzer = stem_text\n",
    "X_counts = vectorizer.fit_transform (file['b'])\n",
    "print(X_counts.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rise of China and the Future of the West</td>\n",
       "      <td>China's rise will inevitably bring the United ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How Russia and China Undermine Democracy</td>\n",
       "      <td>Both Russia and China view weakening Western d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When China Rules the Web</td>\n",
       "      <td>China is set to remake cyberspace in its own i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reeducation Returns to China</td>\n",
       "      <td>It is possible to see how Xinjiang’s reeducati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China Courts Iran</td>\n",
       "      <td>As the Trump administration works out the spec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              h  \\\n",
       "0  The Rise of China and the Future of the West   \n",
       "1      How Russia and China Undermine Democracy   \n",
       "2                      When China Rules the Web   \n",
       "3                  Reeducation Returns to China   \n",
       "4                             China Courts Iran   \n",
       "\n",
       "                                                   b  \n",
       "0  China's rise will inevitably bring the United ...  \n",
       "1  Both Russia and China view weakening Western d...  \n",
       "2  China is set to remake cyberspace in its own i...  \n",
       "3  It is possible to see how Xinjiang’s reeducati...  \n",
       "4  As the Trump administration works out the spec...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(file['col1'])\n",
    "yt=list()\n",
    "for line in y:\n",
    "    tb=TextBlob(line).sentiment\n",
    "    yt.extend(tb)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-1.0, subjectivity=1.0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('life is boring').sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rise of China and the Future of the West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How Russia and China Undermine Democracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When China Rules the Web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reeducation Returns to China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China Courts Iran</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              b\n",
       "0  The Rise of China and the Future of the West\n",
       "1      How Russia and China Undermine Democracy\n",
       "2                      When China Rules the Web\n",
       "3                  Reeducation Returns to China\n",
       "4                             China Courts Iran"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2=pd.DataFrame(file['h'])\n",
    "tokens2.columns=['b']\n",
    "tokens2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-8613d6b70f37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(X_counts.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#print(vectorizer.get_feature_names())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4370\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4371\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4372\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [x]\n",
    "vectorizer = CountVectorizer()\n",
    "X_counts = vectorizer.fit_transform(corpus)\n",
    "#print(X_counts.shape)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(X_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern=\"[a-zA-Z'`éèî]+\", gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "len(stopwords.words('english'))\n",
    "#it is important to note that stopword funtion will only work with tokenized text.\n",
    "#With simple text, it will not provide the desired results.\n",
    "#therefore we need to tokenize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-8ca3ea9dfaa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_sents'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentences'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_sents'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row['sentences']), axis=1)\n",
    "df['tokenized_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokenized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file</td>\n",
       "      <td>[file]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentences tokenized_sents\n",
       "0      file          [file]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'sentences': ['file']})\n",
    "df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row['sentences']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
